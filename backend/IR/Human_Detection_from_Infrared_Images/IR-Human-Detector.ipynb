{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os      \n",
    "import imutils\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Image and Resizing to Standard Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(row_id, root=r\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/Images/\"):     # Path like C:/Users/cook/Desktop/IR/\n",
    "    \n",
    "    filename = \"{}.jpg\".format(row_id)                   # Converts an image number into the file path where the image is located\n",
    "    file_path = os.path.join(root, filename)\n",
    "    imag = Image.open(file_path)                         # Opens the image, and returns the image as a numpy array\n",
    "    img=imag.resize((64,128))                            # Resize the original Image\n",
    "    return np.array(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting HOG Features from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(img):                                                 \n",
    "    \n",
    "    color_features = img.flatten()                                                    # flatten three channel color image\n",
    "    grey_image = rgb2grey(img)                                                        # convert image to greyscale\n",
    "    hog_features = hog(grey_image, block_norm='L2-Hys', pixels_per_cell=(2,2))        # get HOG features from greyscale image\n",
    "    flat_features = np.hstack(hog_features)                                           # combine color and hog features into a single array\n",
    "    return flat_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HOG Features Matrix for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(label_dataframe): \n",
    "    \n",
    "    c=1\n",
    "    features_list = []                                   # List of Features of an Image\n",
    "    for img_id in label_dataframe.index:\n",
    "        img = get_image(img_id)                          # load image\n",
    "        image_features = create_features(img)            # get features for image\n",
    "        features_list.append(image_features)\n",
    "        print(c)                                         # Displays the count of Images converted to Vector\n",
    "        c+=1\n",
    "    feature_matrix = np.array(features_list)             # convert list of arrays into a matrix\n",
    "    return feature_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA to reduce Dimensionality of Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcca(feature_matrix):\n",
    "                                   \n",
    "    print('Feature matrix shape is: ', feature_matrix.shape)           # get shape of feature matrix\n",
    "    ss = StandardScaler()                                              # define standard scaler\n",
    "    human_stand = ss.fit_transform(feature_matrix)                     # run this on our feature matrix\n",
    "    pca = PCA(n_components=50)                                         # set the number of components wisely, maybe less than 75\n",
    "    human_pca = pca.fit_transform(human_stand)                         # use fit_transform to run PCA on our standardized matrix\n",
    "    print('PCA matrix shape is: ', human_pca.shape)                    # look at new shape\n",
    "    return human_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window Mechanism running throughout an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    \n",
    "    for y in range(0, image.shape[0], stepSize):                                  # slide a window across the image\n",
    "        for x in range(0, image.shape[1], stepSize):                              # yield the current window\n",
    "            yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Maximum Suppression for removing False Positive Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    \n",
    "    if len(boxes) == 0:                                                  # if there are no boxes, return an empty list\n",
    "        return []\n",
    "    if boxes.dtype.kind == \"i\":                                          # if the bounding boxes integers, convert them to floats --\n",
    "        boxes = boxes.astype(\"float\")                                    # this is important since we'll be doing a bunch of divisions\n",
    "    pick = []                                                            # initialize the list of picked indexes\n",
    "    x1 = boxes[:,0]                                                      # grab the coordinates of the bounding boxes\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)                                 # compute the area of the bounding boxes \n",
    "    idxs = np.argsort(y2)                                                # sort the bounding boxes by the bottom-right y-coordinate of the bounding box\n",
    "    while len(idxs) > 0:                                                 # keep looping while some indexes still remain in the indexes list\n",
    "        last = len(idxs) - 1                                             # grab the last index in the indexes list  \n",
    "        i = idxs[last]                                                   \n",
    "        pick.append(i)                                                   # add the index value to the list of picked indexes\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])                         # find the largest (x, y) coordinates for the start of the bounding box\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])                         # find the smallest (x, y) coordinates for the end of the bounding box\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)                                 # compute the width of the bounding box\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)                                 # compute the height of the bounding box\n",
    "        overlap = (w * h) / area[idxs[:last]]                            # compute the ratio of overlap\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],                   # delete all indexes from the index list that have\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "    return boxes[pick].astype(\"int\")                                     # return only the bounding boxes that were picked using the integer data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedure for Image Dataset, using Support Vector Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-0430be70df9d>:4: FutureWarning: rgb2grey is deprecated. It will be removed in version 0.19.Please use rgb2gray instead.\n",
      "  grey_image = rgb2grey(img)                                                        # convert image to greyscale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Feature matrix shape is:  (4, 150660)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=50 must be between 0 and min(n_samples, n_features)=4 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b849bb1fce17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/Images/ir.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# Path like C:/Users/cook/Desktop/IR/IR.csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeature_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feature_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m                                                                        \u001b[1;31m# run create_feature_matrix on our dataframe of images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpcca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[1;33m)\u001b[0m                                                                                               \u001b[1;31m# Reduced Matrix of Image Vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m                                       \u001b[1;31m# Access the labels of Training Images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a0d5582458e5>\u001b[0m in \u001b[0;36mpcca\u001b[1;34m(feature_matrix)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhuman_stand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;31m# run this on our feature matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m                                         \u001b[1;31m# set the number of components wisely, maybe less than 75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhuman_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_stand\u001b[0m\u001b[1;33m)\u001b[0m                         \u001b[1;31m# use fit_transform to run PCA on our standardized matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PCA matrix shape is: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhuman_pca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m                    \u001b[1;31m# look at new shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhuman_pca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'full'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'arpack'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'randomized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[1;34m(self, X, n_components)\u001b[0m\n\u001b[0;32m    437\u001b[0m                                  \"if n_samples >= n_features\")\n\u001b[0;32m    438\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             raise ValueError(\"n_components=%r must be between 0 and \"\n\u001b[0m\u001b[0;32m    440\u001b[0m                              \u001b[1;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                              \u001b[1;34m\"svd_solver='full'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=50 must be between 0 and min(n_samples, n_features)=4 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = pd.read_csv(r\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/Images/ir.csv\", index_col=0)     # Path like C:/Users/cook/Desktop/IR/IR.csv\n",
    "feature_matrix = create_feature_matrix(labels)                                                                        # run create_feature_matrix on our dataframe of images\n",
    "hp=pcca(feature_matrix)                                                                                               # Reduced Matrix of Image Vectors\n",
    "X_train = pd.DataFrame(hp)                                 \n",
    "y_train = pd.Series(labels.label.values)                                       # Access the labels of Training Images\n",
    "pd.Series(y_train).value_counts()                                              # look at the distrubution of labels in the train set\n",
    "svm = SVC(kernel='rbf', probability=True, random_state=42)                     # define support vector classifier\n",
    "svm.fit(X_train, y_train)                                                      # fit model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Trained Model to Detect Human in a Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagee = cv2.imread(r\"Complete-Path-Of-Test-Image\")                                         \n",
    "scale_percent = 200                                                                         # percent of original size\n",
    "width = int(imagee.shape[1] * scale_percent / 100)\n",
    "height = int(imagee.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)                                                                       # New dimensions of Image Window\n",
    "image = cv2.resize(imagee, dim, interpolation = cv2.INTER_AREA)                             # resize image\n",
    "(winW , WinH) = (64 , 128)                                                                  # Dimensions of Sliding Window\n",
    "t=[]                                                                                        # Feature Matrix for test image portions\n",
    "k=[]                                                                                        # Indexes corresponding to image portions\n",
    "for (x, y, window) in sliding_window(image, stepSize=16, windowSize=(winW, winH)):          # loop over the sliding window for each layer of the pyramid\n",
    "    if window.shape[0] != winH or window.shape[1] != winW:                                  # if the window does not meet our desired window size, ignore it\n",
    "        continue                                                                                            \n",
    "    clone = image.copy()\n",
    "    op=clone[y:y+winH,x:x+winW]\n",
    "    f=create_features(op)\n",
    "    t.append(f)\n",
    "    k.append([x,y])\n",
    "    cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Window\", clone)\n",
    "    cv2.waitKey(1)\n",
    "    time.sleep(0.025)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE WINDOW\n",
    "# since we do not have a classifier, we'll just draw the window\n",
    "\n",
    "hpp=pcca(t)\n",
    "x_p=pd.DataFrame(hpp)\n",
    "y_p=svm.predict(x_p)\n",
    "pp=list(y_p)                                                                                # Predict the portions where human is detected\n",
    "clon = image.copy()\n",
    "nn=[]                                                                                       # List of indexes of Human Detection Portions\n",
    "for i in range(0,len(pp)):\n",
    "    if pp[i]==1.0:\n",
    "        x=k[i][0]\n",
    "        y=k[i][1]\n",
    "        nn.append([x,y,x+winW,y+winH])\n",
    "        cv2.rectangle(clon, (x, y), (x + winW, y + winH), (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Window\", clon)\n",
    "        cv2.waitKey(50)\n",
    "        time.sleep(0.025)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "ll=non_max_suppression_fast(np.array(nn),0.4)                                              # Applied Non-Max Suppression on Human Detected portions\n",
    "l=list(ll)                                                                                 # List of Indexes of Actual Human Regions \n",
    "clo=image.copy()\n",
    "for kl in l:\n",
    "    cv2.rectangle(clo, (kl[0], kl[1]), (kl[2],kl[3]), (0, 255, 0), 1)                      # Display the most probable Detection Regions out of False Positives\n",
    "    cv2.imshow(\"Window\", clo)\n",
    "    cv2.waitKey(0)\n",
    "    time.sleep(0.025)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\cawin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pytorchyolo import detect, models\n",
    "\n",
    "# Load the YOLO model\n",
    "model = models.load_model(\n",
    "  r\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/cfg/yolov3.cfg\", \n",
    "  r\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/weights/yolov3.weights\")\n",
    "\n",
    "# Load the image as a numpy array\n",
    "img = cv2.imread(\"C:/Users/cawin/CodeProjects/SUTD/Capstone_S15_HTX/IR/Human-Detection-from-Infrared-Images/Images/1.jpg\")\n",
    "\n",
    "# Convert OpenCV bgr to rgb\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Runs the YOLO model on the image \n",
    "boxes = detect.detect_image(model, img)\n",
    "\n",
    "print(boxes)\n",
    "# Output will be a numpy array in the following format:\n",
    "# [[x1, y1, x2, y2, confidence, class]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
